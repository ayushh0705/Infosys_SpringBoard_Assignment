{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\RAVI\n",
      "[nltk_data]     PATHAK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\RAVI\n",
      "[nltk_data]     PATHAK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\RAVI PATHAK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"\n",
    "SpaCy is an NLP library that excels at large-scale text processing. It can be used for tasks such as tokenization, lemmatization, \n",
    "part-of-speech tagging, and named entity recognition. NLTK is another Python library that provides easy-to-use interfaces \n",
    "for over 50 corpora and lexical resources. Both libraries have their strengths, and understanding when to use each is important.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization:\n",
      "['\\nSpaCy is an NLP library that excels at large-scale text processing.', 'It can be used for tasks such as tokenization, lemmatization, \\npart-of-speech tagging, and named entity recognition.', 'NLTK is another Python library that provides easy-to-use interfaces \\nfor over 50 corpora and lexical resources.', 'Both libraries have their strengths, and understanding when to use each is important.']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(paragraph)\n",
    "print(\"Sentence Tokenization:\")\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Tokenization (NLTK):\n",
      "['SpaCy', 'is', 'an', 'NLP', 'library', 'that', 'excels', 'at', 'large-scale', 'text', 'processing', '.', 'It', 'can', 'be', 'used', 'for', 'tasks', 'such', 'as', 'tokenization', ',', 'lemmatization', ',', 'part-of-speech', 'tagging', ',', 'and', 'named', 'entity', 'recognition', '.', 'NLTK', 'is', 'another', 'Python', 'library', 'that', 'provides', 'easy-to-use', 'interfaces', 'for', 'over', '50', 'corpora', 'and', 'lexical', 'resources', '.', 'Both', 'libraries', 'have', 'their', 'strengths', ',', 'and', 'understanding', 'when', 'to', 'use', 'each', 'is', 'important', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(paragraph)\n",
    "print(\"\\nWord Tokenization (NLTK):\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered Words (Stopwords Removed):\n",
      "['SpaCy', 'NLP', 'library', 'excels', 'large-scale', 'text', 'processing', '.', 'used', 'tasks', 'tokenization', ',', 'lemmatization', ',', 'part-of-speech', 'tagging', ',', 'named', 'entity', 'recognition', '.', 'NLTK', 'another', 'Python', 'library', 'provides', 'easy-to-use', 'interfaces', '50', 'corpora', 'lexical', 'resources', '.', 'libraries', 'strengths', ',', 'understanding', 'use', 'important', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "print(\"\\nFiltered Words (Stopwords Removed):\")\n",
    "print(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POS Tagging (NLTK):\n",
      "[('SpaCy', 'NNP'), ('NLP', 'NNP'), ('library', 'JJ'), ('excels', 'NNS'), ('large-scale', 'JJ'), ('text', 'NN'), ('processing', 'NN'), ('.', '.'), ('used', 'VBN'), ('tasks', 'NNS'), ('tokenization', 'NN'), (',', ','), ('lemmatization', 'NN'), (',', ','), ('part-of-speech', 'JJ'), ('tagging', 'NN'), (',', ','), ('named', 'VBN'), ('entity', 'NN'), ('recognition', 'NN'), ('.', '.'), ('NLTK', 'NNP'), ('another', 'DT'), ('Python', 'NNP'), ('library', 'NN'), ('provides', 'VBZ'), ('easy-to-use', 'JJ'), ('interfaces', 'NNS'), ('50', 'CD'), ('corpora', 'NNS'), ('lexical', 'JJ'), ('resources', 'NNS'), ('.', '.'), ('libraries', 'NNS'), ('strengths', 'NNS'), (',', ','), ('understanding', 'VBG'), ('use', 'NN'), ('important', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = pos_tag(filtered_words)\n",
    "print(\"\\nPOS Tagging (NLTK):\")\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatization (SpaCy):\n",
      "Word: \n",
      ", Lemma: \n",
      ", POS: SPACE\n",
      "Word: SpaCy, Lemma: SpaCy, POS: PROPN\n",
      "Word: is, Lemma: be, POS: AUX\n",
      "Word: an, Lemma: an, POS: DET\n",
      "Word: NLP, Lemma: NLP, POS: PROPN\n",
      "Word: library, Lemma: library, POS: NOUN\n",
      "Word: that, Lemma: that, POS: PRON\n",
      "Word: excels, Lemma: excel, POS: VERB\n",
      "Word: at, Lemma: at, POS: ADP\n",
      "Word: large, Lemma: large, POS: ADJ\n",
      "Word: -, Lemma: -, POS: PUNCT\n",
      "Word: scale, Lemma: scale, POS: NOUN\n",
      "Word: text, Lemma: text, POS: NOUN\n",
      "Word: processing, Lemma: processing, POS: NOUN\n",
      "Word: ., Lemma: ., POS: PUNCT\n",
      "Word: It, Lemma: it, POS: PRON\n",
      "Word: can, Lemma: can, POS: AUX\n",
      "Word: be, Lemma: be, POS: AUX\n",
      "Word: used, Lemma: use, POS: VERB\n",
      "Word: for, Lemma: for, POS: ADP\n",
      "Word: tasks, Lemma: task, POS: NOUN\n",
      "Word: such, Lemma: such, POS: ADJ\n",
      "Word: as, Lemma: as, POS: ADP\n",
      "Word: tokenization, Lemma: tokenization, POS: NOUN\n",
      "Word: ,, Lemma: ,, POS: PUNCT\n",
      "Word: lemmatization, Lemma: lemmatization, POS: NOUN\n",
      "Word: ,, Lemma: ,, POS: PUNCT\n",
      "Word: \n",
      ", Lemma: \n",
      ", POS: SPACE\n",
      "Word: part, Lemma: part, POS: NOUN\n",
      "Word: -, Lemma: -, POS: PUNCT\n",
      "Word: of, Lemma: of, POS: ADP\n",
      "Word: -, Lemma: -, POS: PUNCT\n",
      "Word: speech, Lemma: speech, POS: NOUN\n",
      "Word: tagging, Lemma: tagging, POS: NOUN\n",
      "Word: ,, Lemma: ,, POS: PUNCT\n",
      "Word: and, Lemma: and, POS: CCONJ\n",
      "Word: named, Lemma: name, POS: VERB\n",
      "Word: entity, Lemma: entity, POS: NOUN\n",
      "Word: recognition, Lemma: recognition, POS: NOUN\n",
      "Word: ., Lemma: ., POS: PUNCT\n",
      "Word: NLTK, Lemma: NLTK, POS: PROPN\n",
      "Word: is, Lemma: be, POS: AUX\n",
      "Word: another, Lemma: another, POS: DET\n",
      "Word: Python, Lemma: Python, POS: PROPN\n",
      "Word: library, Lemma: library, POS: NOUN\n",
      "Word: that, Lemma: that, POS: PRON\n",
      "Word: provides, Lemma: provide, POS: VERB\n",
      "Word: easy, Lemma: easy, POS: ADV\n",
      "Word: -, Lemma: -, POS: PUNCT\n",
      "Word: to, Lemma: to, POS: ADP\n",
      "Word: -, Lemma: -, POS: PUNCT\n",
      "Word: use, Lemma: use, POS: NOUN\n",
      "Word: interfaces, Lemma: interface, POS: NOUN\n",
      "Word: \n",
      ", Lemma: \n",
      ", POS: SPACE\n",
      "Word: for, Lemma: for, POS: ADP\n",
      "Word: over, Lemma: over, POS: ADP\n",
      "Word: 50, Lemma: 50, POS: NUM\n",
      "Word: corpora, Lemma: corpora, POS: NOUN\n",
      "Word: and, Lemma: and, POS: CCONJ\n",
      "Word: lexical, Lemma: lexical, POS: ADJ\n",
      "Word: resources, Lemma: resource, POS: NOUN\n",
      "Word: ., Lemma: ., POS: PUNCT\n",
      "Word: Both, Lemma: both, POS: DET\n",
      "Word: libraries, Lemma: library, POS: NOUN\n",
      "Word: have, Lemma: have, POS: VERB\n",
      "Word: their, Lemma: their, POS: PRON\n",
      "Word: strengths, Lemma: strength, POS: NOUN\n",
      "Word: ,, Lemma: ,, POS: PUNCT\n",
      "Word: and, Lemma: and, POS: CCONJ\n",
      "Word: understanding, Lemma: understanding, POS: NOUN\n",
      "Word: when, Lemma: when, POS: SCONJ\n",
      "Word: to, Lemma: to, POS: PART\n",
      "Word: use, Lemma: use, POS: VERB\n",
      "Word: each, Lemma: each, POS: PRON\n",
      "Word: is, Lemma: be, POS: AUX\n",
      "Word: important, Lemma: important, POS: ADJ\n",
      "Word: ., Lemma: ., POS: PUNCT\n",
      "Word: \n",
      ", Lemma: \n",
      ", POS: SPACE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nLemmatization (SpaCy):\")\n",
    "for token in doc:\n",
    "    print(f\"Word: {token.text}, Lemma: {token.lemma_}, POS: {token.pos_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Named Entity Recognition (SpaCy):\n",
      "Entity: SpaCy, Label: PERSON\n",
      "Entity: NLP, Label: ORG\n",
      "Entity: NLTK, Label: ORG\n",
      "Entity: over 50, Label: CARDINAL\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNamed Entity Recognition (SpaCy):\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
